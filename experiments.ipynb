{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "58c28645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "73fbbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/jennifer.l/env/lib/python3.8/site-packages (4.64.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/Users/jennifer.l/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e773653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5909eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_log(df, col_name):\n",
    "    \"\"\"Convert column to log space.\n",
    "\n",
    "    Defining log as log(x + EPSILON) to avoid division by zero.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "    col_name : str\n",
    "        Name of column in df to convert to log.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Values of column in log space\n",
    "\n",
    "    \"\"\"\n",
    "    # This is to avoid division by zero while doing np.log10\n",
    "    EPSILON = 1\n",
    "    return np.log10(df[col_name].values + EPSILON)\n",
    "\n",
    "\n",
    "def convert_to_percentile(df, col_name):\n",
    "    \"\"\"Convert column to percentile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "    col_name : str\n",
    "        Name of column in df to convert to percentile.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Column converted to percentile from 1 to 100\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.qcut(df[col_name].rank(method='first'), 100,\n",
    "                   labels=range(1, 101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5958553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for creating features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_dem_features(df):\n",
    "    \"\"\"Get demographic features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of demographic features.\n",
    "\n",
    "    \"\"\"\n",
    "    dem_features = []\n",
    "    prefix = 'dem_'\n",
    "    for col in df.columns:\n",
    "        if prefix == col[:len(prefix)]:\n",
    "            if 'race' not in col:\n",
    "                dem_features.append(col)\n",
    "    return dem_features\n",
    "\n",
    "\n",
    "def get_comorbidity_features(df):\n",
    "    \"\"\"Get comorbidity features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of comorbidity features.\n",
    "\n",
    "    \"\"\"\n",
    "    comorbidity_features = []\n",
    "    comorbidity_sum = 'gagne_sum_tm1'\n",
    "    suffix_elixhauser = '_elixhauser_tm1'\n",
    "    suffix_romano = '_romano_tm1'\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == comorbidity_sum:\n",
    "            comorbidity_features.append(col)\n",
    "        elif suffix_elixhauser == col[-len(suffix_elixhauser):]:\n",
    "            comorbidity_features.append(col)\n",
    "        elif suffix_romano == col[-len(suffix_romano):]:\n",
    "            comorbidity_features.append(col)\n",
    "        else:\n",
    "            continue\n",
    "    return comorbidity_features\n",
    "\n",
    "\n",
    "def get_cost_features(df):\n",
    "    \"\"\"Get cost features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of cost features.\n",
    "\n",
    "    \"\"\"\n",
    "    cost_features = []\n",
    "    prefix = 'cost_'\n",
    "    for col in df.columns:\n",
    "        if prefix == col[:len(prefix)]:\n",
    "            # 'cost_t', 'cost_avoidable_t' are outcomes, not a features\n",
    "            if col not in ['cost_t', 'cost_avoidable_t']:\n",
    "                cost_features.append(col)\n",
    "    return cost_features\n",
    "\n",
    "\n",
    "def get_lab_features(df):\n",
    "    \"\"\"Get lab features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of lab features.\n",
    "\n",
    "    \"\"\"\n",
    "    lab_features = []\n",
    "    suffix_labs_counts = '_tests_tm1'\n",
    "    suffix_labs_low = '-low_tm1'\n",
    "    suffix_labs_high = '-high_tm1'\n",
    "    suffix_labs_normal = '-normal_tm1'\n",
    "    for col in df.columns:\n",
    "        # get lab features\n",
    "        if suffix_labs_counts == col[-len(suffix_labs_counts):]:\n",
    "            lab_features.append(col)\n",
    "        elif suffix_labs_low == col[-len(suffix_labs_low):]:\n",
    "            lab_features.append(col)\n",
    "        elif suffix_labs_high == col[-len(suffix_labs_high):]:\n",
    "            lab_features.append(col)\n",
    "        elif suffix_labs_normal == col[-len(suffix_labs_normal):]:\n",
    "            lab_features.append(col)\n",
    "        else:\n",
    "            continue\n",
    "    return lab_features\n",
    "\n",
    "\n",
    "def get_med_features(df):\n",
    "    \"\"\"Get med features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of med features.\n",
    "\n",
    "    \"\"\"\n",
    "    med_features = []\n",
    "    prefix = 'lasix_'\n",
    "    for col in df.columns:\n",
    "        if prefix == col[:len(prefix)]:\n",
    "            med_features.append(col)\n",
    "    return med_features\n",
    "\n",
    "\n",
    "def get_all_features(df, verbose=False):\n",
    "    \"\"\"Get all features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "    verbose : bool\n",
    "        Print statistics of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of all features.\n",
    "\n",
    "    \"\"\"\n",
    "    dem_features = get_dem_features(df)\n",
    "    comorbidity_features = get_comorbidity_features(df)\n",
    "    cost_features = get_cost_features(df)\n",
    "    lab_features = get_lab_features(df)\n",
    "    med_features = get_med_features(df)\n",
    "\n",
    "    x_column_names = dem_features + comorbidity_features + cost_features + \\\n",
    "                     lab_features + med_features\n",
    "\n",
    "    if verbose:\n",
    "        print('Features breakdown:')\n",
    "        print('   {}: {}'.format('demographic', len(dem_features)))\n",
    "        print('   {}: {}'.format('comorbidity', len(comorbidity_features)))\n",
    "        print('   {}: {}'.format('cost', len(cost_features)))\n",
    "        print('   {}: {}'.format('lab', len(lab_features)))\n",
    "        print('   {}: {}'.format('med', len(med_features)))\n",
    "        print(' {}: {}'.format('TOTAL', len(x_column_names)))\n",
    "\n",
    "    return x_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19b1751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_id(df, id_field='ptid', frac_train=.6):\n",
    "    \"\"\"Split the df by id_field into train/holdout deterministically.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "    id_field : str\n",
    "        Split df by this column (e.g. 'ptid').\n",
    "    frac_train : float\n",
    "        Fraction assigned to train. (1 - frac_train) assigned to holdout.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Data dataframe with additional column 'split' indication train/holdout\n",
    "\n",
    "    \"\"\"\n",
    "    ptid = np.sort(df[id_field].unique())\n",
    "    print(\"Splitting {:,} unique {}\".format(len(ptid), id_field))\n",
    "\n",
    "    # deterministic split\n",
    "    rs = np.random.RandomState(0)\n",
    "    perm_idx = rs.permutation(len(ptid))\n",
    "    num_train = int(frac_train*len(ptid))\n",
    "\n",
    "    # obtain train/holdout\n",
    "    train_idx = perm_idx[:num_train]\n",
    "    holdout_idx  = perm_idx[num_train:]\n",
    "    ptid_train = ptid[train_idx]\n",
    "    ptid_holdout  = ptid[holdout_idx]\n",
    "    print(\" ...splitting by patient: {:,} train, {:,} holdout \".format(\n",
    "      len(ptid_train), len(holdout_idx)))\n",
    "\n",
    "    # make dictionaries\n",
    "    train_dict = {p: \"train\" for p in ptid_train}\n",
    "    holdout_dict  = {p: \"holdout\"  for p in ptid_holdout}\n",
    "    split_dict = {**train_dict, **holdout_dict}\n",
    "\n",
    "    # add train/holdout split to each\n",
    "    split = []\n",
    "    for e in df[id_field]:\n",
    "        split.append(split_dict[e])\n",
    "    df['split'] = split\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e291f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_df():\n",
    "    \"\"\"Load data dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame to use for analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_df = pd.read_csv(\"/Users/jennifer.l/Downloads/dissecting-bias-master-data/data/data_new.csv\")\n",
    "    data_df = data_df.reset_index();\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a89aa1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y_x_df(df, verbose):\n",
    "    \"\"\"Get dataframe with relevant x and Y columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data dataframe.\n",
    "    verbose : bool\n",
    "        Print statistics of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_Y_x_df : pd.DataFrame\n",
    "        Dataframe with x (features) and y (labels) columns\n",
    "    x_column_names : list\n",
    "        List of all x column names (features).\n",
    "    Y_predictors : list\n",
    "        All labels (Y) to predict.\n",
    "\n",
    "    \"\"\"\n",
    "    # cohort columns\n",
    "    cohort_cols = ['index']\n",
    "\n",
    "    # features (x)\n",
    "    x_column_names = get_all_features(df, verbose)\n",
    "\n",
    "    # include log columns\n",
    "    df['log_cost_t'] = convert_to_log(df, 'cost_t')\n",
    "    df['log_cost_avoidable_t'] = convert_to_log(df, 'cost_avoidable_t')\n",
    "\n",
    "    # labels (Y) to predict\n",
    "    Y_predictors = ['gagne_sum_t']\n",
    "\n",
    "    # redefine 'race' variable as indicator\n",
    "    df['dem_race_black'] = np.where(df['race'] == 'black', 1, 0)\n",
    "    df['dem_race_white'] = np.where(df['race'] == 'white', 1, 0)\n",
    "\n",
    "    # additional metrics used for table 2 and table 3\n",
    "    table_metrics = ['dem_race_black', 'dem_race_white', 'risk_score_t', 'program_enrolled_t',\n",
    "                     'cost_t', 'cost_avoidable_t']\n",
    "\n",
    "    # combine all features together -- this forms the Y_x df\n",
    "    all_Y_x_df = df[cohort_cols + x_column_names + Y_predictors + table_metrics].copy()\n",
    "\n",
    "    return all_Y_x_df, x_column_names, Y_predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2ad3f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_columns(df, column_name, quantile):\n",
    "    df['temp'] = \"\"\n",
    "    #df.loc[df[column_name] < df[column_name].quantile(quantile), 'temp'] = int(0)\n",
    "    #df.loc[df[column_name] >= df[column_name].quantile(quantile), 'temp'] = int(1)\n",
    "    \n",
    "    df.loc[df[column_name] < 7, 'temp'] = int(0)\n",
    "    df.loc[df[column_name] >= 7, 'temp'] = int(1)\n",
    "    \n",
    "    df[column_name] = df['temp']\n",
    "    df[column_name] = df[column_name].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "566b9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def debias_weights(original_labels, protected_attributes, multipliers):\n",
    "  exponents = np.zeros(len(original_labels))\n",
    "  for i, m in enumerate(multipliers):\n",
    "    exponents -= m * protected_attributes[i]\n",
    "  weights = np.exp(exponents)/ (np.exp(exponents) + np.exp(-exponents))\n",
    "  weights = np.where(original_labels > 0, 1 - weights, weights)\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9db92bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def get_error_and_violations(y_pred, y, protected_attributes):\n",
    "  acc = np.mean(y_pred != y)\n",
    "  violations = []\n",
    "  for p in protected_attributes:\n",
    "    protected_idxs = np.where(p > 0)\n",
    "    violations.append(np.mean(y_pred) - np.mean(y_pred[protected_idxs]))\n",
    "  pairwise_violations = []\n",
    "# QUESTION: THE FUNCTION OF PAIRWISE VIOLATIONS?\n",
    "  #for i in tqdm(range(len(protected_attributes))):\n",
    "    #for j in range(i+1, len(protected_attributes)):\n",
    "      #protected_idxs = np.where(np.logical_and(protected_attributes[i] > 0, protected_attributes[j] > 0))\n",
    "      #if len(protected_idxs[0]) == 0:\n",
    "        #continue\n",
    "      #pairwise_violations.append(np.mean(y_pred) - np.mean(y_pred[protected_idxs]))\n",
    "  return acc, violations, pairwise_violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4042f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(y_actual, y_hat):\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    OT = sum(y_actual == 1)\n",
    "    OF = sum(y_actual == 0)\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "        \n",
    "    TPR = TP / OT\n",
    "    FPR = FP / OF\n",
    "    TNR = TN / OF\n",
    "    FNR = FN / OT\n",
    "    \n",
    "    \n",
    "    return TPR, FPR, TNR, FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "440afb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression on original dataset\n",
    "def original_logisticRegression(train_df, holdout_df, x_column_names, Y_predictor, protected_groups):\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    X_train = np.array(train_df[x_column_names])\n",
    "    y_train = np.array(train_df[Y_predictor])\n",
    "    X_test = np.array(holdout_df[x_column_names])\n",
    "    y_test = np.array(holdout_df[Y_predictor])\n",
    "    protected_train = [np.array(train_df[g]) for g in protected_groups]\n",
    "    protected_test = [np.array(holdout_df[g]) for g in protected_groups]\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
    "    print(\"Train Accuracy\", acc)\n",
    "    print(\"Train Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
    "    if len(pairwise_violations) > 0:\n",
    "        print(\"Train Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
    "\n",
    "    acc, violations, pairwise_violations = get_error_and_violations(y_pred_test, y_test, protected_test)\n",
    "    print(\"Test Accuracy\", acc)\n",
    "    print(\"Test Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
    "    if len(pairwise_violations) > 0:\n",
    "        print(\"Test Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
    "    \n",
    "    TP, FP, TN, FN = get_performance(y_test, y_pred_test)\n",
    "            \n",
    "    print(\"true positive rate: \", TP)\n",
    "    print(\"false positive rate: \", FP)\n",
    "    print(\"true negative rate: \", TN)\n",
    "    print(\"false negative rate: \", FN)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d988c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debaised_classifier_training(train_df, holdout_df, x_column_names, Y_predictor, protected_groups):\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    X_train = np.array(train_df[x_column_names])\n",
    "    y_train = np.array(train_df[Y_predictor])\n",
    "    X_test = np.array(holdout_df[x_column_names])\n",
    "    y_test = np.array(holdout_df[Y_predictor])\n",
    "    protected_train = [np.array(train_df[g]) for g in protected_groups]\n",
    "    protected_test = [np.array(holdout_df[g]) for g in protected_groups]\n",
    "    \n",
    "    multipliers = np.zeros(len(protected_train))\n",
    "    learning_rate = 1.\n",
    "    n_iters = 100\n",
    "    \n",
    "    test_errs = [0] * 400\n",
    "    test_fair_vio = [0] * 400\n",
    "    \n",
    "    # algorithm 1 in the paper\n",
    "    \n",
    "    for it in tqdm(range(n_iters)):\n",
    "        weights = debias_weights(y_train, protected_train, multipliers)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train, weights)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
    "        multipliers += learning_rate * np.array(violations)\n",
    "\n",
    "        if (it + 1) % n_iters == 0:\n",
    "            print(multipliers)\n",
    "            # c = -1\n",
    "            #for i in tqdm(range(400)):\n",
    "                #print(\"c = \", c)\n",
    "                #weights = debias_weights(y_train, protected_train, c*multipliers)\n",
    "                #model = LogisticRegression()\n",
    "                #model.fit(X_train, y_train, weights)\n",
    "                \n",
    "            y_pred_test = model.predict(X_test)\n",
    "                \n",
    "            acc, violations, pairwise_violations = get_error_and_violations(y_pred_train, y_train, protected_train)\n",
    "            print(\"Train Accuracy\", acc)\n",
    "            print(\"Train Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
    "            if len(pairwise_violations) > 0:\n",
    "                print(\"Train Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
    "\n",
    "            acc, violations, pairwise_violations = get_error_and_violations(y_pred_test, y_test, protected_test)\n",
    "            \n",
    "            TP, FP, TN, FN = get_performance(y_test, y_pred_test)\n",
    "            \n",
    "            print(\"Test Accuracy\", acc)\n",
    "            print(\"true positive rate: \", TP)\n",
    "            print(\"false positive rate: \", FP)\n",
    "            print(\"true negative rate: \", TN)\n",
    "            print(\"false negative rate: \", FN)\n",
    "            \n",
    "            print(\"Test Violation\", max(np.abs(violations)), \" \\t\\t All violations\", violations)\n",
    "            if len(pairwise_violations) > 0:\n",
    "                print(\"Test Intersect Violations\", max(np.abs(pairwise_violations)), \" \\t All violations\", pairwise_violations)\n",
    "\n",
    "                #test_errs[i] = acc\n",
    "                #test_fair_vio[i] = max(np.abs(violations))\n",
    "\n",
    "                print()\n",
    "                print()\n",
    "        \n",
    "                #c += 0.01\n",
    "    \n",
    "    return test_errs, test_fair_vio\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "db677b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load data\n",
    "    data_df = load_data_df()\n",
    "    \n",
    "    print(data_df['cost_avoidable_t'][:100])\n",
    "\n",
    "    # subset to relevant columns\n",
    "    all_Y_x_df, x_column_names, Y_predictors = get_Y_x_df(data_df, verbose=True)\n",
    "    \n",
    "    print(Y_predictors)\n",
    "    \n",
    "    # binarize labels at 55 quantile\n",
    "    for column in Y_predictors:\n",
    "        binarize_columns(all_Y_x_df, column, 0.97)\n",
    "        \n",
    "    print(sum(all_Y_x_df['gagne_sum_t'] == 1))\n",
    "\n",
    "    # assign to 2/3 train, 1/3 holdout\n",
    "    all_Y_x_df = split_by_id(all_Y_x_df, id_field='index',\n",
    "                                   frac_train=.67)\n",
    "\n",
    "    # define train, holdout\n",
    "    # reset_index for pd.concat() along column\n",
    "    train_df = all_Y_x_df[all_Y_x_df['split'] == 'train'].reset_index(drop=True)\n",
    "    holdout_df = all_Y_x_df[all_Y_x_df['split'] == 'holdout'].reset_index(drop=True)\n",
    "    \n",
    "    # dealing with missing data\n",
    "    for column in x_column_names:\n",
    "        train_mean = train_df[column].mean()\n",
    "        train_df[column].fillna(train_mean, inplace=True)\n",
    "        holdout_df[column].fillna(train_mean, inplace=True)\n",
    "    \n",
    "    protected_groups = ['dem_race_black', 'dem_race_white']\n",
    "    \n",
    "    arr_err = []\n",
    "    arr_vio = []\n",
    "    \n",
    "    for column in Y_predictors:\n",
    "        print(\"Processing original logistic model with \", column)\n",
    "        original_logisticRegression(train_df, holdout_df, x_column_names, column, protected_groups)\n",
    "        \n",
    "        print(\"Processing weighted logistic model with \", column)\n",
    "        err, vio = debaised_classifier_training(train_df, holdout_df, x_column_names, column, protected_groups)\n",
    "        \n",
    "        arr_err.append(err)\n",
    "        arr_vio.append(vio)\n",
    "        \n",
    "    #pd.DataFrame(arr_err).to_csv('err_rate_55.csv')\n",
    "    #pd.DataFrame(arr_vio).to_csv('fair_vio_55.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d98f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b0fa4ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "       ...  \n",
      "95       0.0\n",
      "96    2800.0\n",
      "97       0.0\n",
      "98       0.0\n",
      "99       0.0\n",
      "Name: cost_avoidable_t, Length: 100, dtype: float64\n",
      "Features breakdown:\n",
      "   demographic: 8\n",
      "   comorbidity: 34\n",
      "   cost: 13\n",
      "   lab: 90\n",
      "   med: 4\n",
      " TOTAL: 149\n",
      "['gagne_sum_t']\n",
      "1404\n",
      "Splitting 48,784 unique index\n",
      " ...splitting by patient: 32,685 train, 16,099 holdout \n",
      "Processing original logistic model with  gagne_sum_t\n",
      "Train Accuracy 0.0329814899801132\n",
      "Train Violation 0.0162542907514967  \t\t All violations [-0.0162542907514967, 0.0021110125260430414]\n",
      "Test Accuracy 0.03329399341574011\n",
      "Test Violation 0.01622307860523967  \t\t All violations [-0.01622307860523967, 0.002074199135110158]\n",
      "true positive rate:  0.16808510638297872\n",
      "false positive rate:  0.009277624928018428\n",
      "true negative rate:  0.9907223750719816\n",
      "false negative rate:  0.8319148936170213\n",
      "\n",
      "\n",
      "Processing weighted logistic model with  gagne_sum_t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [01:40<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.61083014  0.20920523]\n",
      "Train Accuracy 0.03218601805109377\n",
      "Train Violation 0.01758210231320178  \t\t All violations [-0.01758210231320178, 0.0022834609510059157]\n",
      "Test Accuracy 0.03149263929436611\n",
      "true positive rate:  0.1829787234042553\n",
      "false positive rate:  0.007869985283767356\n",
      "true negative rate:  0.9921300147162326\n",
      "false negative rate:  0.8170212765957446\n",
      "Test Violation 0.016058923084646626  \t\t All violations [-0.016058923084646626, 0.002053211057130454]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9a40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b14904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbee8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1286e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
